{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import prepare_sequence\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "        # never transfer to the start tag and never transfer from stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        # backpointers:(n,m) where n is length of feats and m is tagset_size\n",
    "        # 存储feat中到当前所有tag最优的tag（相当于一步延迟）\n",
    "        # backpointers[i]意味着到feat_i的各个tags的最大可能的tag_pre\n",
    "        backpointers = []\n",
    "\n",
    "        # 初始化一个表示开始前的feat的向量，除了start_tag为0，其余都是-10000，这样由于argmax则feat_0的各个tags都以start_tag为前序\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # 将初始状态寄存在forward_var中，用作后续更新\n",
    "        pre_feat_var = init_vvars[0]\n",
    "\n",
    "        # 对于句子中每一个字的标签特征表示\n",
    "        for feat in feats:\n",
    "            pre_compute_var = torch.stack([pre_feat_var]*self.tagset_size)\n",
    "            crt_feat_var = pre_compute_var + self.transitions\n",
    "            pre_feat_var, best_pre = torch.max(crt_feat_var, dim=1)\n",
    "            backpointers.append(best_pre.tolist())\n",
    "\n",
    "            pre_feat_var += feat\n",
    "\n",
    "        terminal_var = pre_feat_var + \\\n",
    "            self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        total_var, best_tag_id = torch.max(terminal_var, dim=0)\n",
    "\n",
    "        best_path = [best_tag_id.item()]\n",
    "        for backpointer in reversed(backpointers):\n",
    "            best_tag_id = backpointer[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return total_var, best_path\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 初始化，是否需要初始化为-10000还需研究\n",
    "        init_vars = torch.full((1, self.tagset_size), -10000)\n",
    "        init_vars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        #init_vars = torch.zeros((1,self.tagset_size))\n",
    "\n",
    "        # pre_feat:(1,tagset_size),用来保存到前序feat的各个tag的score\n",
    "        pre_feat = init_vars[0]\n",
    "\n",
    "        # 对于每一个feat\n",
    "        for feat in feats:\n",
    "            # 将pre_feat铺为tagset_size*tagset_size，方便计算\n",
    "            pre_compute_var = torch.stack([pre_feat]*self.tagset_size)\n",
    "            # 当前feat的Emission Vector即crt_emit_var[i]为当前feat被标为第i个tag的非归一化概率\n",
    "            crt_emit_var = torch.unsqueeze(feat, dim=0).view(-1, 1)\n",
    "            # 当前feat的总特征向亮=Emission + Transition，crt_feat_var[i][j]为当前feat的前序feat被标为第j个tag的情况下当前feat被标为第i个tag的特征值\n",
    "            crt_feat_var = crt_emit_var+self.transitions\n",
    "            # 迭代计算，因为log(exp(log(exp(\\sum{x}))+y)) = log(exp(\\sum{x})+exp(y))，得到当前feat的总score\n",
    "            feat_var = pre_compute_var + crt_feat_var\n",
    "            # 更新pre_feat，计算logsumexp即到当前feat的各个tag的各种情况的score\n",
    "            pre_feat = feat_var.logsumexp(dim=1)\n",
    "\n",
    "        # 同理，计算到终点的总score，即所有情况（len(feats)^{tagset_size}种）的总score\n",
    "        terminal_var = pre_feat + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        total_score = terminal_var.unsqueeze(dim=0).logsumexp(dim=1)[0]\n",
    "        return total_score\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat(\n",
    "            [torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    # 损失函数，来源于log的最大似然函数\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        total_score = self._forward_alg(feats)\n",
    "        tagged_score = self._score_sentence(feats, tags)\n",
    "        return total_score - tagged_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    START_TAG = \"<START>\"\n",
    "    STOP_TAG = \"<STOP>\"\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 256\n",
    "\n",
    "    training_data = [(\n",
    "        \"参 加 北 京 智 源 大 会\".split(),\n",
    "        \"O O B I I I I I I \".split()\n",
    "    ), (\n",
    "        \"他 出 席 计 算 机 协 会 会 长\".split(),\n",
    "        \"O O O B I I I I O O\".split()\n",
    "    ),\n",
    "        (\n",
    "        \"作 为 主 讲 人 参 加 教 研 会 应 对 疫 情\".split(),\n",
    "        \"O O O O O O O B I I O O O O\".split()\n",
    "    )]\n",
    "\n",
    "    word_to_ix = {}\n",
    "    for sentence, tags in training_data:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "    model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "    # 测试为训练模型\n",
    "    with torch.no_grad():\n",
    "        precheck_sent = prepare_sequence(training_data[-1][0], word_to_ix)\n",
    "        precheck_tags = torch.tensor(\n",
    "            [tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "        print(model(precheck_sent))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        for sentence, tags in training_data[0:-1]:\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = torch.tensor([tag_to_ix[t]\n",
    "                                    for t in tags], dtype=torch.long)\n",
    "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(training_data)):\n",
    "            precheck_sent = prepare_sequence(training_data[i][0], word_to_ix)\n",
    "            print(\"predict:{}\\n target:{}\".format(model(precheck_sent),\n",
    "                                                    prepare_sequence(training_data[i][1], tag_to_ix)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
